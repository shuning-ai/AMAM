# AMAM
The implementation of **AMAM: An Attention-based Multimodal Alignment Model for Medical Visual Question Answering**  

We evaluate our proposal on two public medical VQA datasets: [PathVQA dataset](https://arxiv.org/abs/2003.10286) and [VQA-RAD dataset](https://www.nature.com/articles/sdata2018251#data-citations).

 

## Conditional Reasoning Framework  

We proposes an Attention-based Multimodal Alignment Model (AMAM) for medical VQA, aiming for an alignment of text-based and image-based attention to enrich the textual features.

## Results
AMAM results on PathVQA test set.

|  | Overall | Open-ended | Closed-ended |
| :----:| :----: | :----: | :----: |
| Base | 49.2 | 16.4 | 84.0|
| Our proposal | 50.4 | 18.2 | 84.4| 

AMAM results on VQA-RAD test set.

|  | Overall | Open-ended | Closed-ended |
| :----:| :----: | :----: | :----: |
| Base | 71.6 | 60.0 | 79.3|
| Our proposal | 73.3 | 63.8 | 80.3|

Experimental results on public datasets show the superior performance of AMAM.

## License  
MIT License


## Citation
Please cite following in your publications if they help you


